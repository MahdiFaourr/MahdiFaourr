{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONCahhvR1u6LMtL+nISZjI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiFaourr/MahdiFaourr/blob/main/English_to_French_translate_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repo.\n",
        "!git clone https://github.com/zaka-ai/machine_learning_certification.git\n",
        "# Import necessary libraries and functions.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, TimeDistributed, Dense, Bidirectional\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# Download the 'punkt' tokenizer models\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define paths to English and French data\n",
        "path_to_English_data = \"machine_learning_certification/Challenge 7/en.csv\"\n",
        "path_to_French_data = \"machine_learning_certification/Challenge 7/fr.csv\"\n",
        "\n",
        "# Read English and French data\n",
        "data_English = pd.read_csv(path_to_English_data)\n",
        "data_French = pd.read_csv(path_to_French_data)\n",
        "\n",
        "df=pd.concat([data_English,data_French],axis=1)\n",
        "df.columns=['English','French']\n",
        "\n",
        "# Remove punctuation from English and French sentences\n",
        "def punctuation_remover(text):\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return text\n",
        "\n",
        "df['English'] = df['English'].apply(punctuation_remover)\n",
        "df['French'] = df['French'].apply(punctuation_remover)\n",
        "\n",
        "# Tokenize English and French sentences\n",
        "df['English Tokenize'] = df['English'].apply(word_tokenize)\n",
        "df['French Tokenize'] = df['French'].apply(word_tokenize)\n",
        "\n",
        "# Count the number of unique words in English and French\n",
        "unique_english_words = set()\n",
        "unique_french_words = set()\n",
        "\n",
        "for tokens in df['English Tokenize']:\n",
        "    unique_english_words.update(tokens)\n",
        "number_unique_english_words = len(unique_english_words)\n",
        "\n",
        "for tokens in df['French Tokenize']:\n",
        "    unique_french_words.update(tokens)\n",
        "number_unique_french_words = len(unique_french_words)\n",
        "\n",
        "print(\"The Number of unique English words is:\", number_unique_english_words)\n",
        "print(\"The Number of unique French words is:\", number_unique_french_words)\n",
        "\n",
        "# Tokenize and pad English and French sentences for training\n",
        "tokenizer_En = Tokenizer(num_words=10000)\n",
        "tokenizer_En.fit_on_texts(df['English Tokenize'])\n",
        "x_train = tokenizer_En.texts_to_sequences(df['English Tokenize'])\n",
        "x_train = pad_sequences(x_train, maxlen=30, padding='post')\n",
        "\n",
        "tokenizer_Fr = Tokenizer(num_words=1000)\n",
        "tokenizer_Fr.fit_on_texts(df['French Tokenize'])\n",
        "y_train = tokenizer_Fr.texts_to_sequences(df['French Tokenize'])\n",
        "y_train = pad_sequences(y_train, maxlen=30, padding='post')\n",
        "\n",
        "# Create a sequence-to-sequence model\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=200, output_dim=60, input_length=30))\n",
        "    model.add(Bidirectional(LSTM(60, return_sequences=True)))\n",
        "    model.add(LSTM(75, return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(40, activation='relu')))\n",
        "    model.add(TimeDistributed(Dense(45, activation='relu')))\n",
        "    model.add(TimeDistributed(Dense(345, activation='softmax')))\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, validation_split=0.2, epochs=8, batch_size=32, shuffle=True, verbose=1)\n",
        "\n",
        "# Function to translate English to French\n",
        "def English_to_French_translator(input_text):\n",
        "    input = input_text.translate(str.maketrans('', '', string.punctuation))\n",
        "    input = word_tokenize(input)\n",
        "    input = tokenizer_En.texts_to_sequences([input])\n",
        "    padded_input_sequences = pad_sequences(input, maxlen=30, padding='post')\n",
        "    predictions = model.predict(padded_input_sequences)\n",
        "    translated_sequences = np.argmax(predictions, axis=-1)\n",
        "    translated_text = tokenizer_Fr.sequences_to_texts(translated_sequences)\n",
        "    translated_text = ''.join(translated_text)\n",
        "    return translated_text\n",
        "\n",
        "# Test the translation\n",
        "input_text = \"she is driving the big truck\"\n",
        "translated_text = English_to_French_translator(input_text)\n",
        "print(\"Translated text in French:\", translated_text)\n",
        "\n",
        "# Save the Keras model to a file using pickle\n",
        "with open(\"model.pkl\", \"wb\") as model_file:\n",
        "    pickle.dump(model, model_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "GsanG6Cpe2nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EU6Xapnfe2uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m8gAqsHoe2zY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}